# Content Automation System - Implementation Plan V2
**Last Updated:** November 12, 2025  
**Status:** Foundation Complete, Testing Phase

---

## Executive Summary

An intelligent content assistant that manages blog and LinkedIn content production through email communication. Ben sends brain dumps via email with "CONTENT" in the subject line, the system analyzes the input and generates appropriate content (1-5 pieces), sends it back for approval, and publishes on a schedule using a position-based queue system.

**Key Architecture Decisions:**
- Email processing via **Gmail polling (every 5 minutes)** instead of Pub/Sub webhooks
- Blog publishing via **direct Supabase database writes** (no separate API)
- **Shared Supabase database** between content system and company website
- Cron-based scheduling with **CRON_SECRET security**
- Queue system based on **positions (1, 2, 3...)** not dates

---

## Current Project Status

### ‚úÖ COMPLETED & VERIFIED

#### Infrastructure
- [x] Local development environment set up
- [x] Git repository initialized and pushed to GitHub
- [x] Vercel project connected to GitHub
- [x] Supabase database project created (shared with website)
- [x] Node.js project with all dependencies installed

#### Database
- [x] All content system tables created in Supabase:
  - `content_library` (with pgvector support)
  - `conversation_threads`
  - `content_versions`
  - `assistant_memory`
- [x] Database helper functions (`shift_queue_positions`, `match_content`)
- [x] Indexes created for performance
- [x] Verified connection to existing `blog_posts` table (from website)

#### Authentication & APIs
- [x] Gmail API enabled and OAuth2 configured
- [x] Gmail refresh token obtained and tested
- [x] Google Sheets API enabled
- [x] Service account created and JSON key stored
- [x] Supabase credentials configured
- [x] Anthropic API key configured

#### Testing
- [x] Gmail send/receive tested successfully
- [x] Google Sheets read/write tested (wrote to cell A6 on all 3 tabs)
- [x] Supabase CRUD operations tested
- [x] Environment variables loaded correctly

#### Core Files
- [x] Queue management library (`lib/queue.js`)
- [x] Email utility helper (`lib/email.js`)
- [x] Google Sheets utility (`lib/sheets.js`)
- [x] TypeScript types defined (`types/index.ts`)
- [x] Updated `package.json` with all dependencies

### üî® BUILT BUT NOT TESTED

#### API Endpoints
- [ ] `/api/content/generate` - Content generation endpoint
- [ ] `/api/email/poll` - Gmail polling cron (every 5 minutes)
- [ ] `/api/publish/blog` - Blog publishing (direct DB write)
- [ ] `/api/publish/linkedin` - LinkedIn publishing (API call)
- [ ] `/api/assistant/lineup` - Monday morning lineup email

#### Security
- [x] CRON_SECRET environment variable configured
- [x] All cron endpoints secured with auth header check
- [ ] Not tested in production yet

### ‚è≥ NOT STARTED

#### LinkedIn Integration
- [ ] LinkedIn API "Share on LinkedIn" product approval (pending)
- [ ] LinkedIn access token generation (waiting for approval)
- [ ] LinkedIn publishing endpoint testing

#### Email Processing
- [ ] Approval parsing logic (revision requests, swapping)
- [ ] Queue management through email commands ("B1 next", "show more")
- [ ] Revision loop implementation

#### Advanced Features
- [ ] Semantic search for past content
- [ ] Vector embeddings generation
- [ ] Style matching based on past content
- [ ] Assistant memory system
- [ ] Proactive content suggestions
- [ ] Google Sheets status tracking integration

---

## System Architecture

### Email Flow (Gmail Polling)

**Decision:** Using Gmail API polling every 5 minutes instead of Google Cloud Pub/Sub
**Reason:** Pub/Sub is unreliable and complex to configure; polling is simple and sufficient

```
Every 5 minutes ‚Üí `/api/email/poll` cron runs
                  ‚Üì
         Gmail API query: from:ben@corradoco.com subject:CONTENT is:unread
                  ‚Üì
         Process each unread email
                  ‚Üì
         Check if reply to existing thread OR new content request
                  ‚Üì
         Generate content OR handle approval
                  ‚Üì
         Send approval email back to Ben
                  ‚Üì
         Mark email as read
```

**Email Requirements:**
- **From:** ben@corradoco.com (hardcoded filter)
- **Subject:** Must contain "CONTENT" (case insensitive)
- **Body:** Brain dump, story, insight, or approval response

### Content Generation Logic

**CRITICAL:** The system must intelligently determine how many pieces of content to create based on input substance. It does NOT fabricate or stretch content.

**Content Prompt (Final Version):**
```
You are an intelligent content assistant for Ben Corrado, founder of Corrado & Co., 
a Denver-based automation consulting company.

CONTEXT:
- Ben sends you brain dumps, stories, insights, or quick ideas via email
- Your job is to transform these into polished blog posts and LinkedIn content
- You're part of a content automation system that helps Ben maintain consistent output

BEN'S VOICE & EXPERTISE:
- Automation consultant specializing in n8n, Zapier, custom code workflows
- Works with mid-sized companies (20-100 employees)
- Values efficiency, excellence, systematic approaches
- Writes in a professional but approachable tone - technical when needed, 
  accessible when possible, honest and realistic
- Uses real examples from client work
- Focuses on practical, actionable insights

YOUR TASK:
1. ANALYZE the input to determine what content can legitimately be created from it
2. DO NOT fabricate or stretch content too far beyond what the input supports
3. Generate between 1-5 total pieces based on substance:
   - Minimum: 1 LinkedIn post (always possible)
   - Maximum: 2 blog posts + 3 LinkedIn posts

CONTENT GUIDELINES:
- Blog posts: 800-1200 words, deeper dives, technical depth, real examples
- LinkedIn posts: 75-200 words, conversational, single insight or story, actionable

ASSESSMENT CRITERIA:
- Quick tip/hack = 1 LinkedIn post
- Single story/insight = 1 LinkedIn post or 1 blog
- Detailed case study = 1 blog + 1-2 LinkedIn posts
- Multiple insights = 2-3 LinkedIn posts (different angles)
- Major project/learning = 1-2 blogs + 2-3 LinkedIn posts
```

**Output Format:**
```json
{
  "assessment": "Brief explanation of what you decided and why",
  "blog": [
    {
      "title": "...",
      "content": "...",
      "excerpt": "..."
    }
  ],
  "linkedin": [
    { "content": "..." }
  ]
}
```

### Blog Publishing Architecture

**Decision:** Direct database writes to shared `blog_posts` table instead of separate API
**Reason:** Website and content system share the same Supabase database - no need for HTTP overhead

```
Monday 10am cron ‚Üí `/api/publish/blog`
                    ‚Üì
           Get content at queue position 1
                    ‚Üì
           Generate slug from title
                    ‚Üì
           INSERT directly into `blog_posts` table
                    ‚Üì
           UPDATE `content_library` status to 'posted'
                    ‚Üì
           Shift remaining queue positions up
                    ‚Üì
           Website automatically shows new blog post
```

**Table Relationship:**
- `content_library` table: Content system's staging/queue
- `blog_posts` table: Website's production table (shared)
- When published, content is INSERTED into `blog_posts` and marked as 'posted' in `content_library`

### Queue System

**Key Concept:** Content lives in numbered positions (1, 2, 3...) not scheduled dates. This allows flexible reordering.

**Queue Operations:**
- `addToQueue(contentId, type)` - Adds to end of queue
- `swapWithNext(contentId, type)` - Swaps with next item
- `moveToPosition(contentId, type, position)` - Moves to specific position
- `removeFromQueue(contentId)` - Removes from queue
- `getQueue(type, limit)` - Gets queue contents
- `getQueueHealth()` - Checks if queues are healthy

**Publishing Process:**
1. Cron job runs at scheduled time
2. Pull content at position 1
3. Publish to platform
4. Update status to 'posted'
5. Remove from queue (set position to NULL)
6. Call `shift_queue_positions()` to move everyone up

**Email Commands (Planned):**
- "B1 next" - Swap blog at position 1 with position 2
- "L2 - trim intro" - Request revision on LinkedIn post 2
- "Show more blogs" - Display positions 3-5
- "All approved" - Approve all pieces in lineup

---

## Database Schema (Final)

### Content System Tables

```sql
-- Core content storage with vector embeddings
CREATE TABLE content_library (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title TEXT,
    content TEXT NOT NULL,
    type VARCHAR(20) CHECK (type IN ('blog', 'linkedin')),
    status VARCHAR(20) DEFAULT 'draft',
    version INTEGER DEFAULT 1,
    posted_date TIMESTAMPTZ,
    queue_position INTEGER, -- Position in queue (NULL if not queued)
    embedding vector(1536), -- OpenAI embeddings (NOT IMPLEMENTED YET)
    tags TEXT[],
    style_notes TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Thread management for email conversations
CREATE TABLE conversation_threads (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_id UUID REFERENCES content_library(id),
    email_thread_id TEXT, -- Gmail thread ID
    current_version INTEGER DEFAULT 1,
    status VARCHAR(20),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Version history for content iterations
CREATE TABLE content_versions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_id UUID REFERENCES content_library(id),
    thread_id UUID REFERENCES conversation_threads(id),
    version_number INTEGER,
    content TEXT,
    feedback TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Assistant's long-term memory (NOT IMPLEMENTED YET)
CREATE TABLE assistant_memory (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    memory_type VARCHAR(50),
    content JSONB,
    valid_from TIMESTAMPTZ DEFAULT NOW(),
    valid_until TIMESTAMPTZ
);

-- Indexes
CREATE INDEX ON content_library USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX idx_queue_position ON content_library(type, queue_position) 
  WHERE queue_position IS NOT NULL;
CREATE INDEX idx_content_type_status ON content_library(type, status);
CREATE INDEX idx_email_thread ON conversation_threads(email_thread_id);

-- Helper function: Shift queue positions after publish
CREATE OR REPLACE FUNCTION shift_queue_positions(
  content_type VARCHAR(20),
  from_position INT
)
RETURNS void
LANGUAGE sql
AS $$
  UPDATE content_library
  SET queue_position = queue_position - 1
  WHERE type = content_type 
  AND queue_position > from_position;
$$;

-- Helper function: Vector similarity search (NOT TESTED YET)
CREATE OR REPLACE FUNCTION match_content(
  query_embedding vector(1536),
  match_threshold float,
  match_count int
)
RETURNS TABLE (
  id uuid,
  title text,
  content text,
  type varchar(20),
  similarity float
)
LANGUAGE sql STABLE
AS $$
  SELECT
    id,
    title,
    content,
    type,
    1 - (embedding <=> query_embedding) as similarity
  FROM content_library
  WHERE 1 - (embedding <=> query_embedding) > match_threshold
  ORDER BY embedding <=> query_embedding
  LIMIT match_count;
$$;
```

### Website Table (Existing - Shared)

```sql
-- Blog posts table (existing on website, used by content system for publishing)
CREATE TABLE blog_posts (
  id uuid DEFAULT gen_random_uuid() PRIMARY KEY,
  created_at timestamptz DEFAULT now() NOT NULL,
  updated_at timestamptz DEFAULT now() NOT NULL,
  title text NOT NULL,
  slug text UNIQUE NOT NULL,
  content text NOT NULL,
  excerpt text,
  published boolean DEFAULT false NOT NULL,
  published_at timestamptz
);

-- Auto-update trigger (already exists on website)
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = now();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_blog_posts_updated_at 
BEFORE UPDATE ON blog_posts 
FOR EACH ROW 
EXECUTE FUNCTION update_updated_at_column();
```

---

## Environment Variables (Final)

### Required Variables

```env
# Supabase (shared with website)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key

# Anthropic AI
ANTHROPIC_API_KEY=sk-ant-...

# Google Gmail API
GMAIL_CLIENT_ID=xxxxx.apps.googleusercontent.com
GMAIL_CLIENT_SECRET=xxxxx
GMAIL_REFRESH_TOKEN=1//04xxxxx
EMAIL_ADDRESS=ben@corradoco.com

# Google Sheets API
GOOGLE_SHEET_ID=your-sheet-id-from-url

# Cron Security
CRON_SECRET=your-randomly-generated-secret-64-chars

# LinkedIn API (PENDING APPROVAL)
LINKEDIN_CLIENT_ID=xxxxx
LINKEDIN_CLIENT_SECRET=xxxxx
LINKEDIN_ACCESS_TOKEN=xxxxx
```

### Variable Notes

**CRON_SECRET:**
- Generated via: `node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"`
- Used to secure all cron endpoints
- Vercel automatically includes this in Authorization header when calling cron jobs
- Each endpoint checks: `if (authHeader !== 'Bearer ${process.env.CRON_SECRET}') return 401`

**GOOGLE_SHEET_ID:**
- Found in Google Sheet URL: `docs.google.com/spreadsheets/d/{SHEET_ID}/edit`
- Service account must have Editor access to this sheet

**Service Account:**
- JSON key file stored in `/private/google-service-account.json`
- NOT in environment variables (loaded directly in code)
- Added to `.gitignore` for security

---

## File Structure

```
content-system/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ content/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate.js          ‚úÖ Content generation with intelligent analysis
‚îÇ   ‚îú‚îÄ‚îÄ email/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ poll.js              üî® Gmail polling cron (every 5 min)
‚îÇ   ‚îú‚îÄ‚îÄ publish/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ blog.js              üî® Direct DB write to blog_posts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ linkedin.js          ‚è≥ LinkedIn API (pending approval)
‚îÇ   ‚îú‚îÄ‚îÄ assistant/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lineup.js            üî® Monday morning lineup email
‚îÇ   ‚îî‚îÄ‚îÄ health.js                ‚úÖ Simple health check endpoint
‚îÇ
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ queue.js                 ‚úÖ Queue management functions
‚îÇ   ‚îú‚îÄ‚îÄ email.js                 ‚úÖ Gmail sending utilities
‚îÇ   ‚îú‚îÄ‚îÄ sheets.js                ‚úÖ Google Sheets utilities
‚îÇ   ‚îú‚îÄ‚îÄ supabase.ts              ‚úÖ Supabase client
‚îÇ   ‚îî‚îÄ‚îÄ anthropic.ts             ‚úÖ Anthropic client
‚îÇ
‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                 ‚úÖ TypeScript type definitions
‚îÇ
‚îú‚îÄ‚îÄ private/
‚îÇ   ‚îî‚îÄ‚îÄ google-service-account.json  ‚úÖ Service account key (gitignored)
‚îÇ
‚îú‚îÄ‚îÄ test-gmail.js                ‚úÖ Gmail send/receive test
‚îú‚îÄ‚îÄ test-sheets.js               ‚úÖ Google Sheets write test
‚îú‚îÄ‚îÄ test-supabase.js             ‚úÖ Supabase CRUD test
‚îú‚îÄ‚îÄ get-gmail-token.js           ‚úÖ One-time token generator
‚îÇ
‚îú‚îÄ‚îÄ .env.local                   ‚úÖ Local environment variables
‚îú‚îÄ‚îÄ .gitignore                   ‚úÖ Ignores node_modules, .env, private/
‚îú‚îÄ‚îÄ package.json                 ‚úÖ All dependencies installed
‚îú‚îÄ‚îÄ tsconfig.json                ‚úÖ TypeScript configuration
‚îú‚îÄ‚îÄ vercel.json                  ‚úÖ Cron jobs configured
‚îú‚îÄ‚îÄ plan.txt                     ‚úÖ Original plan
‚îî‚îÄ‚îÄ plan_v2.txt                  ‚úÖ This document

Legend:
‚úÖ = Complete and tested
üî® = Built but not tested
‚è≥ = Not started
```

---

## Cron Jobs Schedule

```json
{
  "crons": [
    {
      "path": "/api/email/poll",
      "schedule": "*/5 * * * *"
    },
    {
      "path": "/api/publish/blog",
      "schedule": "0 10 * * 1"
    },
    {
      "path": "/api/publish/linkedin",
      "schedule": "0 9 * * 2"
    },
    {
      "path": "/api/publish/linkedin",
      "schedule": "0 9 * * 4"
    },
    {
      "path": "/api/publish/linkedin",
      "schedule": "0 9 * * 6"
    },
    {
      "path": "/api/assistant/lineup",
      "schedule": "0 8 * * 1"
    }
  ]
}
```

**Schedule Breakdown:**
- **Every 5 minutes:** Check Gmail for new emails
- **Monday 8am:** Send weekly lineup approval email
- **Monday 10am:** Publish blog post
- **Tuesday 9am:** Publish LinkedIn post
- **Thursday 9am:** Publish LinkedIn post
- **Saturday 9am:** Publish LinkedIn post

**Security:** All cron endpoints verify `Authorization: Bearer ${CRON_SECRET}` header

---

## Critical Implementation Details

### 1. Content Generation Intelligence

The system MUST analyze input before generating. It does not blindly create 1 blog + 2 LinkedIn posts every time.

**Bad Approach (Don't Do This):**
```javascript
// ‚ùå Always generates fixed amount regardless of input
const generated = {
  blog: { title: "...", content: "..." },
  linkedin: [{ content: "..." }, { content: "..." }]
};
```

**Correct Approach:**
```javascript
// ‚úÖ Analyzes input first, then decides
const prompt = `
  Analyze input: "${input}"
  
  How much substance? What can be created without fabricating?
  Generate 1-5 pieces based on actual content value.
`;

const generated = {
  assessment: "This is a quick tip, so 1 LinkedIn post is appropriate",
  blog: [],
  linkedin: [{ content: "..." }]
};
```

### 2. Email Filtering

The Gmail polling endpoint filters on TWO criteria:

```javascript
// Both conditions must be true:
1. Email from: ben@corradoco.com
2. Subject contains: "CONTENT" (case insensitive)

// Gmail API query:
q: `from:ben@corradoco.com subject:CONTENT is:unread after:${tenMinutesAgo}`
```

If either is false, email is ignored.

### 3. Blog Publishing Flow

**IMPORTANT:** Do NOT create a separate blog API. Write directly to the shared database.

```javascript
// ‚ùå Wrong: Making HTTP request to separate API
const response = await fetch('https://website.com/api/blog', {
  method: 'POST',
  body: JSON.stringify(blogData)
});

// ‚úÖ Correct: Direct database write
const { data: publishedPost } = await supabase
  .from('blog_posts')
  .insert({
    title: content.title,
    slug: generateSlug(content.title),
    content: content.content,
    excerpt: content.excerpt || content.content.substring(0, 200) + '...',
    published: true,
    published_at: new Date().toISOString()
  })
  .select()
  .single();
```

### 4. Queue Position Management

After publishing, ALWAYS shift positions:

```javascript
// After publishing content at position 1:

// 1. Update content status
await supabase
  .from('content_library')
  .update({
    status: 'posted',
    posted_date: new Date().toISOString(),
    queue_position: null  // Remove from queue
  })
  .eq('id', content.id);

// 2. Shift everyone else up
await supabase.rpc('shift_queue_positions', {
  content_type: 'blog',
  from_position: 1
});

// Result: Position 2 becomes 1, Position 3 becomes 2, etc.
```

### 5. Service Account vs OAuth

**Gmail API:** Uses OAuth2 with refresh token
**Google Sheets API:** Uses service account with JSON key

```javascript
// Gmail (OAuth)
const oauth2Client = new google.auth.OAuth2(...);
oauth2Client.setCredentials({
  refresh_token: process.env.GMAIL_REFRESH_TOKEN
});

// Sheets (Service Account)
const serviceAccount = require('../private/google-service-account.json');
const auth = new google.auth.GoogleAuth({
  credentials: serviceAccount,
  scopes: ['https://www.googleapis.com/auth/spreadsheets']
});
```

---

## Next Steps (Priority Order)

### Phase 1: Core Functionality Testing (Next)

1. **Test Content Generation**
   - Send email to ben@corradoco.com with subject "CONTENT test"
   - Wait 5 minutes for polling
   - Verify approval email arrives
   - Check content_library table for entries

2. **Test Approval Flow**
   - Reply to approval email with "approved"
   - Verify content moves to 'approved' status
   - Verify content added to queue

3. **Test Queue Management**
   - Manually add content to queue
   - Verify positions are correct
   - Test swapping, moving

4. **Test Blog Publishing**
   - Add approved blog to position 1
   - Trigger `/api/publish/blog` manually
   - Verify blog appears in blog_posts table
   - Check website to confirm it shows

### Phase 2: LinkedIn Integration (Blocked)

**Waiting on:** LinkedIn "Share on LinkedIn" API approval

Once approved:
1. Get access token
2. Test LinkedIn publishing endpoint
3. Add content to queue and test automated posting

### Phase 3: Enhanced Features

1. **Revision Loop**
   - Parse feedback from email replies
   - Generate revised content
   - Track version history

2. **Queue Commands**
   - "B1 next" - swap blog
   - "L2 next" - swap LinkedIn
   - "Show more" - display deep queue
   - Parse and execute these commands

3. **Monday Lineup**
   - Test lineup email generation
   - Show next 4 pieces
   - Show what's coming after

4. **Google Sheets Integration**
   - Log content to Status sheet
   - Update status in real-time
   - Archive posted content

### Phase 4: Advanced Intelligence

1. **Semantic Search**
   - Generate embeddings for all content
   - Implement similarity search
   - "Make it like the X post" command

2. **Style Matching**
   - Extract style markers from past content
   - Apply to new content generation

3. **Proactive Assistant**
   - Analyze content gaps
   - Suggest topics
   - Strategic check-ins

---

## Testing Checklist

### Manual Testing Commands

```bash
# Test health endpoint
curl https://your-project.vercel.app/api/health

# Test Gmail polling (requires CRON_SECRET)
curl -X GET https://your-project.vercel.app/api/email/poll \
  -H "Authorization: Bearer YOUR_CRON_SECRET"

# Test blog publishing (requires CRON_SECRET)
curl -X GET https://your-project.vercel.app/api/publish/blog \
  -H "Authorization: Bearer YOUR_CRON_SECRET"

# Test content generation
curl -X POST https://your-project.vercel.app/api/content/generate \
  -H "Content-Type: application/json" \
  -d '{"input": "Test content generation"}'
```

### Database Verification Queries

```sql
-- Check all content
SELECT * FROM content_library ORDER BY created_at DESC;

-- Check queue status
SELECT type, queue_position, title, status 
FROM content_library 
WHERE queue_position IS NOT NULL 
ORDER BY type, queue_position;

-- Check conversation threads
SELECT * FROM conversation_threads;

-- Check what's been published
SELECT * FROM blog_posts ORDER BY published_at DESC;
```

---

## Known Limitations & Future Considerations

### Current Limitations

1. **No OpenAI Integration Yet**
   - Embeddings generation not implemented
   - Semantic search not functional
   - Will need OPENAI_API_KEY when ready

2. **LinkedIn Pending**
   - Can't test LinkedIn publishing until API approved
   - Access token generation blocked

3. **No Revision Logic**
   - Can approve content
   - Cannot request specific changes yet
   - Version history not being used

4. **No Queue Commands**
   - Can't swap from email yet
   - Can't view deep queue
   - Manual database updates required

5. **Google Sheets Not Connected**
   - Status tracking not live
   - Archive not being populated
   - Training examples not being loaded

### Future Enhancements

1. **Multi-account Support**
   - Currently hardcoded to ben@corradoco.com
   - Could support multiple users/companies

2. **Media Attachments**
   - LinkedIn posts with images
   - Blog posts with featured images
   - Process attachments from emails

3. **Analytics Dashboard**
   - Content performance tracking
   - Queue health monitoring
   - Publishing history

4. **Advanced Scheduling**
   - Specific date scheduling
   - Time zone handling
   - Holiday skipping

5. **Webhook Notifications**
   - Slack notifications
   - Discord updates
   - SMS alerts

---

## Troubleshooting Guide

### Gmail Polling Not Working

**Symptoms:** Emails sent but no response
**Check:**
1. Vercel logs for cron execution
2. Email subject contains "CONTENT"
3. Email from ben@corradoco.com
4. CRON_SECRET set in Vercel
5. Gmail API quota in Google Cloud Console

**Debug:**
```bash
# Check Vercel logs
vercel logs --follow

# Manually trigger polling
curl -X GET https://your-project.vercel.app/api/email/poll \
  -H "Authorization: Bearer $CRON_SECRET"
```

### Content Not Generating

**Symptoms:** Email processed but no content created
**Check:**
1. Anthropic API key valid
2. Supabase connection working
3. Check Vercel logs for errors
4. Verify JSON parsing isn't failing

**Debug:**
```sql
-- Check if content was created
SELECT * FROM content_library ORDER BY created_at DESC LIMIT 5;

-- Check conversation threads
SELECT * FROM conversation_threads ORDER BY created_at DESC LIMIT 5;
```

### Blog Not Publishing

**Symptoms:** Cron runs but blog doesn't appear
**Check:**
1. Content at queue position 1 with status 'approved'
2. Slug generation working (no duplicates)
3. blog_posts table exists and is accessible
4. CRON_SECRET correct

**Debug:**
```sql
-- Check queue
SELECT * FROM content_library 
WHERE type = 'blog' AND queue_position = 1 AND status = 'approved';

-- Check if blog was published
SELECT * FROM blog_posts ORDER BY created_at DESC LIMIT 1;
```

### Cron Jobs Not Running

**Symptoms:** Scheduled jobs never execute
**Check:**
1. Vercel Pro subscription active
2. vercel.json has correct crons configuration
3. CRON_SECRET environment variable set
4. Deployment successful

**Solution:**
- Check Vercel dashboard ‚Üí Cron Jobs section
- Verify crons are listed and enabled
- Check deployment logs for errors

---

## Success Metrics (Goals)

- **Efficiency:** <5 min from email to draft
- **Quality:** <20% revision rate
- **Queue Health:** Always 1+ blog, 3+ LinkedIn ready
- **Approval Speed:** <2 rounds for approval
- **Publishing:** Zero missed publishing slots
- **Time Saved:** 10+ hours/week on content

---

## Key Contacts & Resources

**Project Owner:** Ben Corrado (ben@corradoco.com)  
**Company:** Corrado & Co. (Denver-based automation consulting)  
**GitHub Repo:** cordocode/content-system  
**Vercel Project:** content-system  
**Supabase Project:** (shared with company website)

**External APIs:**
- Gmail API: https://developers.google.com/gmail/api
- LinkedIn API: https://docs.microsoft.com/en-us/linkedin/
- Anthropic API: https://docs.anthropic.com/
- Google Sheets API: https://developers.google.com/sheets/api

**Important Documentation:**
- Vercel Cron Jobs: https://vercel.com/docs/cron-jobs
- Supabase pgvector: https://supabase.com/docs/guides/ai/vector-columns
- Gmail API Node.js: https://github.com/googleapis/google-api-nodejs-client

---

## Final Notes for Future AI Assistants

1. **Never fabricate content beyond the input** - The intelligent analysis is the core value prop
2. **Always use direct DB writes for blog** - Don't create a separate API
3. **Gmail polling is intentional** - Pub/Sub was tried and rejected
4. **CRON_SECRET is mandatory** - All cron endpoints must check it
5. **Queue positions, not dates** - The system is position-based for flexibility
6. **ben@corradoco.com is hardcoded** - This is by design, not a bug
7. **Service account for Sheets, OAuth for Gmail** - Different auth methods for different APIs
8. **LinkedIn is pending** - Can't test until API approved by LinkedIn
9. **Shared Supabase database** - Content system and website use the same DB
10. **Test locally before deploying** - Use the test scripts to verify everything works

This system is designed to be Ben's personal content assistant. Keep it focused, intelligent, and reliable. Quality over quantity. Substance over fabrication.

---

**Document Version:** 2.0  
**Last Updated:** November 12, 2025  
**Status:** Ready for testing phase