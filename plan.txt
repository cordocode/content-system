# Content Automation System - Complete Build Plan

## Executive Summary
An intelligent content assistant that manages blog and LinkedIn content production through email communication, leveraging semantic memory for context-aware content generation, with Google Sheets tracking and automated publishing. Uses a **next-up queue system** where content moves through positions rather than being assigned specific dates, allowing flexible swapping and reordering during Monday approval sessions.

## System Architecture Overview
- **Frontend**: None (email-based interface)
- **Backend**: Vercel-hosted API endpoints
- **Database**: Supabase (PostgreSQL + pgvector) - **SHARED** with company website
- **Content Memory**: Vector embeddings for semantic search
- **Tracking**: Google Sheets for status/scheduling
- **Publishing**: Direct Supabase writes (blog) + LinkedIn API
- **AI**: Claude API for content generation and conversation

### Database Architecture Note
The content system and the company website share the same Supabase database project. This allows:
- Direct writes from content system to website's `blog_posts` table
- No separate blog API needed
- Immediate publishing with no HTTP overhead
- Simplified error handling

The content system uses its own tables (`content_library`, `conversation_threads`, etc.) alongside the website's existing `blog_posts` table.

### Content Status Flow
```
draft (created) ‚Üí queued (added to queue) ‚Üí approved (ready to publish) ‚Üí posted (published)
                ‚Üò revision (needs changes) ‚Üó
```

### Queue System
- Content lives in numbered positions (1, 2, 3...)
- Publishing pulls from position 1
- After publishing, all positions shift up
- User can swap positions during approval
- "Next" always refers to position +1

---

## Core Flow Examples

### Flow 1: Brain Dump ‚Üí Content Creation
```
1. User emails: "Just closed a big automation deal with a construction company..."
2. System parses email ‚Üí determines: 1 blog, 2 LinkedIn posts
3. Claude generates all content in single call (ensuring variety)
4. Logs to Google Sheet with proposed schedule
5. Sends approval email immediately (queue is low)
6. User approves ‚Üí Updates sheet ‚Üí Queues for posting
```

### Flow 2: Monday Approval Request with Queue Management
```
1. Monday 8am: System sends weekly content lineup for approval
2. Email format:
   [CA-789] Weekly Content Lineup
   
   üìù BLOG (Monday): [B1]
   Title: "How We Saved Fleet 400 Hours..."
   [First 200 words...]
   ‚Üí Next in queue: "3 Automation Truths"
   
   üíº LINKEDIN (Tuesday): [L1] 
   "Stop writing prompts like..."
   [Full post...]
   ‚Üí Next in queue: "My favorite debugging trick"
   
   üíº LINKEDIN (Thursday): [L2]
   "My favorite prompt hack..."
   [Full post...]
   ‚Üí Next in queue: "The invoice automation story"
   
   üíº LINKEDIN (Saturday): [L3]
   "3 Hard Truths About Automation..."
   [Full post...]
   ‚Üí Next in queue: "Document processing insights"
   
3. User replies: "B1 next, L2 - trim the middle section, rest approved"
4. System parses: 
   - Swaps B1 with next blog in queue
   - Revises L2, keeps in position
   - Approves L1, L3
5. Updates queue positions and statuses
6. Confirms changes: "Blog swapped to '3 Automation Truths', L2 revision coming..."
```

### Flow 3: Revision Loop
```
1. System sends: "[CA-789] Blog post for approval"
2. User replies: "Too technical, make it more accessible"
3. System adjusts content, maintains version history
4. Replies with V2: "Simplified technical language, added analogies"
5. User: "approved"
6. System updates status ‚Üí schedules post
```

### Flow 4: Reference Past Content
```
1. User: "Create a LinkedIn post similar to the automation invoice one but for document retrieval"
2. System searches embeddings for "automation invoice" post
3. Extracts style markers and structure
4. Generates new content with similar tone/format
5. Sends for approval with note: "Based on your invoice automation post style"
```

### Flow 5: Automated Publishing from Queue
```
1. Tuesday 8am: Cron job triggers LinkedIn publish
2. Pulls next approved LinkedIn post from queue (position 1)
3. Formats using LinkedIn template from Google Sheets
4. Posts via LinkedIn API
5. Removes from queue, shifts others up (position 2 ‚Üí 1, etc.)
6. Updates Google Sheet status to "posted"
7. If queue empty, sends alert email
```

### Flow 6: Queue Swapping Example
```
1. User receives Monday lineup with blog about "Fleet case study"
2. User replies: "B1 next, what else do we have for blogs?"
3. System:
   - Swaps position 1 ‚Üî position 2 in blog queue
   - Shows positions 3-5 from blog queue
4. Replies: "Swapped to '3 Automation Truths'. Other options:
   - Position 3: 'Invoice Processing Deep Dive'
   - Position 4: 'Why I Built My Own CRM'
   - Position 5: 'The Hidden Cost of Manual Work'"
5. User: "Actually swap with position 4"
6. System moves position 4 ‚Üí 1, shifts others accordingly
7. Monday's blog is now "Why I Built My Own CRM"
```

---

## Database Schema

### Supabase Tables

#### Content System Tables (New)

```sql
-- Core content storage with vector embeddings
CREATE TABLE content_library (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title TEXT,
    content TEXT NOT NULL,
    type VARCHAR(20) CHECK (type IN ('blog', 'linkedin')),
    status VARCHAR(20) DEFAULT 'draft',
    version INTEGER DEFAULT 1,
    posted_date TIMESTAMP,
    queue_position INTEGER, -- Position in queue (NULL if not queued)
    embedding vector(1536), -- OpenAI embeddings
    tags TEXT[],
    style_notes TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Thread management for email conversations
CREATE TABLE conversation_threads (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_id UUID REFERENCES content_library(id),
    email_thread_id TEXT, -- Gmail thread ID
    current_version INTEGER DEFAULT 1,
    status VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Version history for content iterations
CREATE TABLE content_versions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_id UUID REFERENCES content_library(id),
    thread_id UUID REFERENCES conversation_threads(id),
    version_number INTEGER,
    content TEXT,
    feedback TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Assistant's long-term memory
CREATE TABLE assistant_memory (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    memory_type VARCHAR(50), -- 'weekly_vision', 'style_preference', etc.
    content JSONB,
    valid_from TIMESTAMP DEFAULT NOW(),
    valid_until TIMESTAMP
);

-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create indexes
CREATE INDEX ON content_library USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX idx_queue_position ON content_library(type, queue_position) WHERE queue_position IS NOT NULL;
```

#### Website Tables (Existing - Reference Only)

```sql
-- Blog posts table (existing on website, used by content system for publishing)
CREATE TABLE blog_posts (
  id uuid DEFAULT gen_random_uuid() PRIMARY KEY,
  created_at timestamptz DEFAULT now() NOT NULL,
  updated_at timestamptz DEFAULT now() NOT NULL,
  title text NOT NULL,
  slug text UNIQUE NOT NULL,
  content text NOT NULL,
  excerpt text,
  published boolean DEFAULT false NOT NULL,
  published_at timestamptz
);

-- Auto-update the updated_at timestamp on post edits
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = now();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_blog_posts_updated_at 
BEFORE UPDATE ON blog_posts 
FOR EACH ROW 
EXECUTE FUNCTION update_updated_at_column();
```

---

## Step-by-Step Build Plan

### Phase 1: Foundation (Week 1)

#### Step 1.1: Set up Supabase
```bash
# Initialize Supabase project
npm install @supabase/supabase-js
```

- Use existing Supabase project (shared with website)
- Enable pgvector extension (if not already enabled)
- Create content system tables from schema above
- Set up RLS policies for security
- Note: `blog_posts` table already exists from website

#### Step 1.2: Create Vercel API Structure
```
/api
  /email
    /receive.ts      # Webhook for incoming emails
    /send.ts         # Send approval/update emails
  /content
    /generate.ts     # Claude content generation
    /approve.ts      # Handle approval responses
    /retrieve.ts     # Semantic search endpoint
  /publish
    /blog.ts         # Blog publishing (direct DB write)
    /linkedin.ts     # LinkedIn publishing
  /assistant
    /vision.ts       # Monday check-ins
    /memory.ts       # Memory management
```

#### Step 1.3: Google Sheets Integration
```javascript
// Install Google Sheets API
npm install googleapis

// Set up service account authentication
// Create sheets service for reading/writing
```

- Set up service account in Google Cloud Console
- Share Google Sheet with service account email
- Create helper functions for sheet operations

---

### Phase 2: Email Processing (Week 1-2)

#### Step 2.1: Gmail API Setup
```javascript
// OAuth2 setup for Gmail
const auth = new google.auth.OAuth2(
  CLIENT_ID,
  CLIENT_SECRET,
  REDIRECT_URL
);

// Watch for incoming emails
await gmail.users.watch({
  userId: 'me',
  requestBody: {
    topicName: 'projects/PROJECT_ID/topics/gmail-push',
    labelIds: ['INBOX']
  }
});
```

#### Step 2.2: Email Parser with Thread Context
```javascript
// api/email/receive.ts
export async function handleIncomingEmail(email) {
  // Extract thread ID
  const threadId = email.threadId;
  
  // Check if part of existing conversation
  const existingThread = await supabase
    .from('conversation_threads')
    .select('*')
    .eq('email_thread_id', threadId)
    .single();
  
  if (existingThread) {
    // Handle as revision/approval
    return handleRevision(email, existingThread);
  } else {
    // New content request
    return handleNewContent(email);
  }
}
```

#### Step 2.3: Approval Response Parser
```javascript
async function parseApprovalResponse(emailBody, thread) {
  const prompt = `
    Analyze this email response to a content approval request.
    Thread context: ${JSON.stringify(thread)}
    
    Determine the user's intent:
    - "approved" / "good" ‚Üí status: approved
    - "B1 next" / "L2 next" ‚Üí swap with next in queue
    - "trim intro" / specific feedback ‚Üí status: revision, extract feedback
    - "skip" ‚Üí remove from queue
    
    Return JSON:
    {
      "action": "approve|revise|swap|skip",
      "feedback": "specific changes requested",
      "contentId": "which content piece (B1, L1, etc)"
    }
  `;
  
  const response = await claude.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 500,
    messages: [{ role: 'user', content: prompt }]
  });
  
  return JSON.parse(response.content[0].text);
}
```

---

### Phase 3: Content Generation (Week 2)

#### Step 3.1: Claude Content Generator
```javascript
// api/content/generate.ts
import Anthropic from '@anthropic-ai/sdk';
import { createClient } from '@supabase/supabase-js';

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_KEY
);

export async function generateContent(emailBody, threadContext = null) {
  // Load training examples from Google Sheets
  const examples = await loadTrainingExamples();
  
  // Build context-aware prompt
  const systemPrompt = `
    You are a content creation assistant specializing in automation consulting content.
    
    Style guidelines:
    - Blog posts: 800-1200 words, technical but accessible, use real examples
    - LinkedIn posts: 150-300 words, conversational, include actionable insights
    
    Past examples for reference:
    ${JSON.stringify(examples.slice(0, 3))}
  `;
  
  const userPrompt = `
    Based on this input: "${emailBody}"
    
    Generate:
    1. One blog post (title + full content)
    2. Two LinkedIn posts (different angles/hooks)
    
    Ensure variety in:
    - Hook styles (question vs statement vs story)
    - Content depth (tactical vs strategic)
    - Tone (technical vs accessible)
    
    Format as JSON:
    {
      "blog": {
        "title": "...",
        "content": "...",
        "excerpt": "..."
      },
      "linkedin": [
        { "content": "...", "hook": "..." },
        { "content": "...", "hook": "..." }
      ]
    }
  `;
  
  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 4000,
    system: systemPrompt,
    messages: [{ role: 'user', content: userPrompt }]
  });
  
  const generated = JSON.parse(response.content[0].text);
  
  // Store in content_library
  const contentIds = await storeGeneratedContent(generated);
  
  // Generate embeddings for semantic search
  await generateEmbeddings(contentIds);
  
  return { generated, contentIds };
}
```

#### Step 3.2: Embedding Generation
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function generateEmbeddings(contentIds) {
  for (const id of contentIds) {
    const { data: content } = await supabase
      .from('content_library')
      .select('title, content')
      .eq('id', id)
      .single();
    
    const textToEmbed = `${content.title}\n\n${content.content}`;
    
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: textToEmbed
    });
    
    const embedding = response.data[0].embedding;
    
    await supabase
      .from('content_library')
      .update({ embedding })
      .eq('id', id);
  }
}
```

---

### Phase 4: Semantic Search (Week 2-3)

#### Step 4.1: Content Retrieval
```javascript
// api/content/retrieve.ts
export async function searchSimilarContent(query, contentType = null, limit = 5) {
  // Generate embedding for search query
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query
  });
  
  const queryEmbedding = response.data[0].embedding;
  
  // Search using pgvector cosine similarity
  let searchQuery = supabase
    .from('content_library')
    .select('*')
    .not('embedding', 'is', null);
  
  if (contentType) {
    searchQuery = searchQuery.eq('type', contentType);
  }
  
  // Use RPC function for vector similarity search
  const { data } = await supabase.rpc('match_content', {
    query_embedding: queryEmbedding,
    match_threshold: 0.7,
    match_count: limit
  });
  
  return data;
}

// Create this function in Supabase:
/*
CREATE OR REPLACE FUNCTION match_content(
  query_embedding vector(1536),
  match_threshold float,
  match_count int
)
RETURNS TABLE (
  id uuid,
  title text,
  content text,
  type varchar(20),
  similarity float
)
LANGUAGE sql STABLE
AS $$
  SELECT
    id,
    title,
    content,
    type,
    1 - (embedding <=> query_embedding) as similarity
  FROM content_library
  WHERE 1 - (embedding <=> query_embedding) > match_threshold
  ORDER BY embedding <=> query_embedding
  LIMIT match_count;
$$;
*/
```

---

### Phase 5: Queue Management (Week 3)

#### Step 5.1: Queue Operations
```javascript
// lib/queue.ts
export async function addToQueue(contentId, type) {
  // Get current highest position
  const { data: maxPos } = await supabase
    .from('content_library')
    .select('queue_position')
    .eq('type', type)
    .not('queue_position', 'is', null)
    .order('queue_position', { ascending: false })
    .limit(1);
  
  const nextPosition = maxPos?.[0]?.queue_position + 1 || 1;
  
  await supabase
    .from('content_library')
    .update({ 
      queue_position: nextPosition,
      status: 'queued' 
    })
    .eq('id', contentId);
  
  return nextPosition;
}

export async function swapQueuePositions(contentId, type, swapWith = 'next') {
  const { data: current } = await supabase
    .from('content_library')
    .select('queue_position')
    .eq('id', contentId)
    .single();
  
  const targetPosition = swapWith === 'next' 
    ? current.queue_position + 1 
    : swapWith;
  
  const { data: target } = await supabase
    .from('content_library')
    .select('id, queue_position')
    .eq('type', type)
    .eq('queue_position', targetPosition)
    .single();
  
  if (!target) return null;
  
  // Swap positions
  await supabase
    .from('content_library')
    .update({ queue_position: targetPosition })
    .eq('id', contentId);
  
  await supabase
    .from('content_library')
    .update({ queue_position: current.queue_position })
    .eq('id', target.id);
  
  return { current: contentId, target: target.id };
}

export async function getQueueContents(type, limit = 10) {
  const { data } = await supabase
    .from('content_library')
    .select('*')
    .eq('type', type)
    .not('queue_position', 'is', null)
    .order('queue_position', { ascending: true })
    .limit(limit);
  
  return data;
}
```

#### Step 5.2: Position Shifting
```javascript
// Create RPC function for shifting positions after publish
/*
CREATE OR REPLACE FUNCTION shift_queue_positions(
  content_type varchar(20),
  from_position int
)
RETURNS void
LANGUAGE sql
AS $$
  UPDATE content_library
  SET queue_position = queue_position - 1
  WHERE type = content_type 
  AND queue_position > from_position;
$$;
*/
```

---

### Phase 6: Publishing (Week 3-4)

#### Step 6.1: Blog Publishing (Direct Database Write)
```javascript
// api/publish/blog.ts
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_KEY
);

function generateSlug(title) {
  return title
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/^-+|-+$/g, '');
}

export default async function handler(req, res) {
  try {
    // Get next blog post from queue (position 1)
    const { data: content } = await supabase
      .from('content_library')
      .select('*')
      .eq('type', 'blog')
      .eq('status', 'approved')
      .eq('queue_position', 1)
      .single();
    
    if (!content) {
      await notifyEmptyQueue('blog');
      return res.status(200).json({ message: 'No blog post in queue' });
    }
    
    // Generate slug from title
    const slug = generateSlug(content.title);
    
    // Prepare excerpt (first 200 chars or AI-generated)
    const excerpt = content.excerpt || content.content.substring(0, 200) + '...';
    
    // Write directly to blog_posts table
    const { data: publishedPost, error: publishError } = await supabase
      .from('blog_posts')
      .insert({
        title: content.title,
        slug: slug,
        content: content.content,
        excerpt: excerpt,
        published: true,
        published_at: new Date().toISOString()
      })
      .select()
      .single();
    
    if (publishError) throw publishError;
    
    // Update content_library status
    await supabase
      .from('content_library')
      .update({
        status: 'posted',
        posted_date: new Date().toISOString(),
        queue_position: null
      })
      .eq('id', content.id);
    
    // Shift remaining queue positions up
    await supabase.rpc('shift_queue_positions', {
      content_type: 'blog',
      from_position: 1
    });
    
    // Update Google Sheet
    await updateGoogleSheet(content.id, 'posted', new Date());
    
    console.log(`‚úÖ Published blog: "${content.title}" to website`);
    
    res.status(200).json({ 
      success: true, 
      published: publishedPost 
    });
    
  } catch (error) {
    console.error('Blog publishing error:', error);
    
    // Don't remove from queue on error - will retry next cycle
    await sendEmail(`
      ‚ö†Ô∏è Failed to publish blog post
      Error: ${error.message}
      Will retry on next scheduled publish cycle.
    `);
    
    res.status(500).json({ error: error.message });
  }
}
```

#### Step 6.2: LinkedIn Publishing (API)
```javascript
// api/publish/linkedin.ts
import axios from 'axios';

export default async function handler(req, res) {
  try {
    // Get next LinkedIn post from queue (position 1)
    const { data: content } = await supabase
      .from('content_library')
      .select('*')
      .eq('type', 'linkedin')
      .eq('status', 'approved')
      .eq('queue_position', 1)
      .single();
    
    if (!content) {
      await notifyEmptyQueue('linkedin');
      return res.status(200).json({ message: 'No LinkedIn post in queue' });
    }
    
    // Get user profile (LinkedIn URN)
    const profileResponse = await axios.get('https://api.linkedin.com/v2/me', {
      headers: {
        'Authorization': `Bearer ${process.env.LINKEDIN_ACCESS_TOKEN}`,
        'Content-Type': 'application/json'
      }
    });
    
    const authorUrn = `urn:li:person:${profileResponse.data.id}`;
    
    // Post to LinkedIn
    const postResponse = await axios.post(
      'https://api.linkedin.com/v2/ugcPosts',
      {
        author: authorUrn,
        lifecycleState: 'PUBLISHED',
        specificContent: {
          'com.linkedin.ugc.ShareContent': {
            shareCommentary: {
              text: content.content
            },
            shareMediaCategory: 'NONE'
          }
        },
        visibility: {
          'com.linkedin.ugc.MemberNetworkVisibility': 'PUBLIC'
        }
      },
      {
        headers: {
          'Authorization': `Bearer ${process.env.LINKEDIN_ACCESS_TOKEN}`,
          'Content-Type': 'application/json',
          'X-Restli-Protocol-Version': '2.0.0'
        }
      }
    );
    
    // Update content_library status
    await supabase
      .from('content_library')
      .update({
        status: 'posted',
        posted_date: new Date().toISOString(),
        queue_position: null
      })
      .eq('id', content.id);
    
    // Shift remaining queue positions up
    await supabase.rpc('shift_queue_positions', {
      content_type: 'linkedin',
      from_position: 1
    });
    
    // Update Google Sheet
    await updateGoogleSheet(content.id, 'posted', new Date());
    
    console.log(`‚úÖ Published LinkedIn post`);
    
    res.status(200).json({ 
      success: true, 
      postId: postResponse.data.id 
    });
    
  } catch (error) {
    console.error('LinkedIn publishing error:', error);
    
    await sendEmail(`
      ‚ö†Ô∏è Failed to publish LinkedIn post
      Error: ${error.message}
      Will retry on next scheduled publish cycle.
    `);
    
    res.status(500).json({ error: error.message });
  }
}
```

#### Step 6.3: Unified Publishing Handler
```javascript
// Shared publishing logic
async function publishFromQueue(type) {
  try {
    // Get next approved content from position 1
    const { data: content } = await supabase
      .from('content_library')
      .select('*')
      .eq('type', type)
      .eq('status', 'approved')
      .eq('queue_position', 1)
      .single();
    
    if (!content) {
      await notifyEmptyQueue(type);
      return null;
    }
    
    // Publish based on type
    if (type === 'blog') {
      await publishBlog(content);
    } else if (type === 'linkedin') {
      await publishLinkedIn(content);
    }
    
    // Update status to posted
    await supabase
      .from('content_library')
      .update({
        status: 'posted',
        posted_date: new Date().toISOString(),
        queue_position: null
      })
      .eq('id', content.id);
    
    // Shift remaining items up in queue
    await supabase.rpc('shift_queue_positions', {
      content_type: type,
      from_position: content.queue_position
    });
    
    // Update Google Sheet
    await updateGoogleSheet(content.id, 'posted', new Date());
    
    // Log successful posting
    console.log(`Published: ${type} - "${content.title}" at ${new Date()}`);
    
  } catch (error) {
    console.error(`Publishing error for ${type}:`, error);
    await logError(error);
    
    // Notify about failure but don't remove from queue
    await sendEmail(`Failed to publish ${type} content. Will retry next cycle.`);
  }
}

// Individual cron jobs for each publishing slot
export const mondayBlogPublisher = async () => {
  await publishFromQueue('blog');
};

export const tuesdayLinkedInPublisher = async () => {
  await publishFromQueue('linkedin');
};

export const thursdayLinkedInPublisher = async () => {
  await publishFromQueue('linkedin');
};

export const saturdayLinkedInPublisher = async () => {
  await publishFromQueue('linkedin');
};

// Alert when queue is running low
async function notifyEmptyQueue(type) {
  const queueStatus = await checkQueueHealth();
  
  if (queueStatus[`needs${type.charAt(0).toUpperCase() + type.slice(1)}`]) {
    await sendEmail(`
      ‚ö†Ô∏è ${type} queue is empty!
      
      Next scheduled post has no content.
      Please send new content or the ${type} slot will be skipped.
    `);
  }
}
```

---

### Phase 7: Proactive Assistant (Week 4)

#### Step 7.1: Monday Morning Lineup
```javascript
// api/assistant/lineup.ts
export default async function handler(req, res) {
  try {
    // Get next 4 pieces (1 blog, 3 LinkedIn) from queue
    const { data: nextBlog } = await supabase
      .from('content_library')
      .select('*')
      .eq('type', 'blog')
      .eq('queue_position', 1)
      .single();
    
    const { data: nextLinkedIn } = await supabase
      .from('content_library')
      .select('*')
      .eq('type', 'linkedin')
      .in('queue_position', [1, 2, 3])
      .order('queue_position', { ascending: true });
    
    // Get next items for context
    const { data: blogQueue } = await getQueueContents('blog', 5);
    const { data: linkedInQueue } = await getQueueContents('linkedin', 5);
    
    // Build approval email
    const emailBody = `
      üìÖ Weekly Content Lineup
      
      üìù BLOG (Monday): [B1]
      Title: ${nextBlog?.title || 'No blog in queue'}
      ${nextBlog ? nextBlog.content.substring(0, 200) + '...' : ''}
      ‚Üí Next: ${blogQueue[1]?.title || 'None'}
      
      üíº LINKEDIN (Tuesday): [L1]
      ${nextLinkedIn[0]?.content || 'No post in queue'}
      ‚Üí Next: ${linkedInQueue[1]?.title || 'None'}
      
      üíº LINKEDIN (Thursday): [L2]
      ${nextLinkedIn[1]?.content || 'No post in queue'}
      ‚Üí Next: ${linkedInQueue[2]?.title || 'None'}
      
      üíº LINKEDIN (Saturday): [L3]
      ${nextLinkedIn[2]?.content || 'No post in queue'}
      ‚Üí Next: ${linkedInQueue[3]?.title || 'None'}
      
      ---
      Reply with:
      - "All approved" to confirm
      - "B1 next" to swap blog
      - "L2 - [feedback]" to request changes
      - "Show more [type]" to see deeper queue
    `;
    
    await sendEmail(emailBody, '[CA] Weekly Content Lineup');
    
    res.status(200).json({ success: true });
  } catch (error) {
    console.error('Lineup generation error:', error);
    res.status(500).json({ error: error.message });
  }
}
```

#### Step 7.2: Context-Aware Suggestions
```javascript
async function suggestContent(context) {
  // Analyze recent conversations, memory, and gaps in content
  const recentContent = await supabase
    .from('content_library')
    .select('title, tags, type')
    .order('created_at', { ascending: false })
    .limit(10);
  
  const { data: memory } = await supabase
    .from('assistant_memory')
    .select('*')
    .eq('memory_type', 'weekly_vision')
    .order('valid_from', { ascending: false })
    .limit(1);
  
  const prompt = `
    Based on:
    - Recent content: ${JSON.stringify(recentContent)}
    - Weekly goals: ${JSON.stringify(memory)}
    - Context: ${context}
    
    Suggest 3 content ideas (1 blog, 2 LinkedIn) that:
    1. Fill gaps in recent coverage
    2. Align with goals
    3. Build on existing momentum
    
    Return as JSON with rationale for each.
  `;
  
  const response = await claude.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 1000,
    messages: [{ role: 'user', content: prompt }]
  });
  
  return JSON.parse(response.content[0].text);
}
```

---

### Phase 8: Google Sheets Integration (Throughout)

#### Step 8.1: Sheet Structure
```javascript
// Sheet tabs:
// 1. Status - tracking pipeline (columns: INPUT, TYPE, ID, CONTENT, QUEUE_POSITION, STATUS, POSTED_DATE)
// 2. Formatting - output templates  
// 3. Examples - training examples
// 4. Archive - posted content log

const SHEET_ID = 'your-sheet-id';
const sheets = google.sheets({ version: 'v4', auth });

async function updateContentStatus(contentId, status, row, queuePosition = null) {
  const values = queuePosition 
    ? [[queuePosition, status, new Date().toISOString()]]
    : [[null, status, new Date().toISOString()]];
    
  await sheets.spreadsheets.values.update({
    spreadsheetId: SHEET_ID,
    range: `Status!E${row}:G${row}`, // Queue Position, Status, Timestamp
    valueInputOption: 'RAW',
    requestBody: { values }
  });
}

async function logPostedContent(content) {
  // Add to Archive tab when posted
  await sheets.spreadsheets.values.append({
    spreadsheetId: SHEET_ID,
    range: 'Archive!A:E',
    valueInputOption: 'RAW',
    requestBody: {
      values: [[
        content.title,
        content.type,
        content.posted_date,
        content.content.substring(0, 100) + '...',
        content.id
      ]]
    }
  });
}
```

#### Step 8.2: Training Examples Loader
```javascript
async function loadTrainingExamples() {
  const response = await sheets.spreadsheets.values.get({
    spreadsheetId: SHEET_ID,
    range: 'Examples!A:H'
  });
  
  return response.data.values.slice(1).map(row => ({
    input: row[0],
    date: row[1],
    density: row[2],
    blog1: row[3],
    blog2: row[4],
    linkedin1: row[5],
    linkedin2: row[6],
    linkedin3: row[7]
  }));
}
```

---

## Environment Variables

```env
# Supabase (shared with website)
SUPABASE_URL=your-supabase-url
SUPABASE_SERVICE_KEY=your-service-key

# APIs
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-claude-key
LINKEDIN_CLIENT_ID=your-linkedin-client-id
LINKEDIN_CLIENT_SECRET=your-linkedin-client-secret
LINKEDIN_ACCESS_TOKEN=your-linkedin-token

# Google
GOOGLE_SERVICE_ACCOUNT=base64-encoded-json
GOOGLE_SHEET_ID=your-sheet-id

# Email
GMAIL_CLIENT_ID=your-client-id
GMAIL_CLIENT_SECRET=your-client-secret
GMAIL_REFRESH_TOKEN=your-refresh-token
EMAIL_ADDRESS=your-email@gmail.com
```

## Vercel Cron Configuration

```json
// vercel.json
{
  "cron": [
    {
      "path": "/api/publish/blog",
      "schedule": "0 10 * * 1"  // Monday 10am
    },
    {
      "path": "/api/publish/linkedin",
      "schedule": "0 9 * * 2"   // Tuesday 9am
    },
    {
      "path": "/api/publish/linkedin",
      "schedule": "0 9 * * 4"   // Thursday 9am
    },
    {
      "path": "/api/publish/linkedin",
      "schedule": "0 9 * * 6"   // Saturday 9am
    },
    {
      "path": "/api/assistant/lineup",
      "schedule": "0 8 * * 1"   // Monday 8am - Weekly lineup
    }
  ]
}
```

---

## Testing Plan

### Week 5: Integration Testing

1. **Email Flow Test**
   - Send test brain dump email
   - Verify content generation
   - Test approval/revision loop
   - Confirm sheet updates

2. **Queue Management Test**
   - Test "B1 next" swapping
   - Verify position shifting after publish
   - Test "show more" deep queue viewing
   - Confirm queue reordering logic
   - Test empty queue alerts

3. **Semantic Search Test**
   - Reference past content by name
   - Test similarity matching
   - Verify context retrieval

4. **Publishing Test**
   - Test blog direct DB write to blog_posts table
   - Verify LinkedIn API posting
   - Test queue pulling (position 1)
   - Verify position shifting post-publish
   - Test empty queue handling
   - Check status updates in both tables

---

## Monitoring & Maintenance

### Dashboards to Build
1. Content pipeline status (Supabase dashboard)
2. Queue health monitor
3. Publishing log viewer
4. Error log viewer

### Regular Tasks
- Weekly: Review assistant memory updates
- Monthly: Review content patterns
- Quarterly: Retrain on new examples

---

## Next Steps Priority

1. **Immediate (Day 1-3)**
   - Set up Supabase with content system tables
   - Create Vercel project structure
   - Configure Gmail API

2. **Short-term (Week 1)**
   - Build email parser
   - Implement basic content generation
   - Set up Google Sheets connection

3. **Medium-term (Week 2-3)**
   - Add semantic search
   - Build approval workflow
   - Implement queue management
   - Test blog direct writes

4. **Long-term (Week 4+)**
   - Add proactive assistant features
   - Optimize context retrieval
   - Build performance analytics

---

## Success Metrics

- **Efficiency**: <5 min from email to draft
- **Quality**: <20% revision rate  
- **Queue Health**: Always 1+ blog, 3+ LinkedIn ready
- **Approval Speed**: <2 rounds for approval
- **Publishing**: Zero missed publishing slots
- **Time Saved**: 10+ hours/week on content

---

## Notes

- Start with email ‚Üí generation ‚Üí approval flow
- Add intelligence incrementally
- Test each component in isolation
- Keep context windows optimized (<8k tokens)
- Log everything for debugging
- Blog posts write directly to shared blog_posts table
- No separate blog API needed

## Quick Reference: Email Commands

### Monday Lineup Responses
- **"All approved"** - Lock in all 4 pieces
- **"B1 next"** - Skip to next blog in queue
- **"L2 next"** - Skip to next LinkedIn in queue  
- **"L1 - trim intro"** - Request specific revision
- **"Show more blogs"** - See positions 3-5
- **"What else for LinkedIn?"** - See deeper queue
- **"B1 skip, rest approved"** - Remove one, approve others
- **"Move position 4 blog to top"** - Specific reordering

### General Content Requests
- **"Make it like the prompt hacking post"** - Style matching
- **"Create 2 LinkedIn posts about [topic]"** - New content
- **"What's in the blog queue?"** - View upcoming content
- **"Need more technical content"** - Strategic guidance